{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Steps to implement the LR:\n",
    "##### 1. Read the Dataset .\n",
    "##### 2. Clean the dataset\n",
    "##### 3. Init the variables. \n",
    "##### 4. Normalize the dataset.\n",
    "##### 5. Predict the values.\n",
    "##### 6. Finding out the error.\n",
    "##### 7. Find out the changes required using Gradient Descent.\n",
    "##### 8. Apply changes to weights.\n",
    "##### 9. Repeat the process untill error is not converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every thing all right\n"
     ]
    }
   ],
   "source": [
    "#All required imports\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "import random\n",
    "from math import sqrt\n",
    "print('Every thing all right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_name):\n",
    "    dataset = list()\n",
    "    file = reader(file_name)\n",
    "    for row in file :\n",
    "        if not row:\n",
    "            continue\n",
    "        dataset.append(row)\n",
    "    for i in range(dataset[0]):\n",
    "        dataset = str_column_to_float(dataset, i)\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_column_to_float(dataset, index):\n",
    "    for row in dataset:\n",
    "        row[index] = float(row[index].strip())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49257296555513885, 0.8745028817099862, 0.3787708768511092, 0.5076524029116535, 0.48570325372694956]\n"
     ]
    }
   ],
   "source": [
    "def init_weights(no_of_col):\n",
    "    weights = list()\n",
    "    weights = [random.random() for _ in range(no_of_col)]\n",
    "    return weights\n",
    "print(init_weights(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 6], [2, 5]]\n"
     ]
    }
   ],
   "source": [
    "def min_max(dataset):\n",
    "    min_max = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        min_val = min([row[i] for row in dataset])\n",
    "        max_val = max([row[i] for row in dataset])\n",
    "        min_max.append([min_val, max_val])\n",
    "    return min_max\n",
    "dataset = [[1, 2], [5, 3], [6, 5]]\n",
    "print(min_max(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.0, -1.0], [0.2, -0.3333333333333333], [1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(dataset):\n",
    "    min_max_list = min_max(dataset)\n",
    "    for i in range(len(dataset[0])):\n",
    "        for row in dataset:\n",
    "            row[i] = row[i] - (min_max_list[i][1]+min_max_list[i][0])/2\n",
    "            if row[i] != 0.0 :\n",
    "                row[i] = row[i]*2/(min_max_list[i][1] - min_max_list[i][0])\n",
    "    return dataset\n",
    "dataset = [[1, 2], [4, 3], [6, 5]]\n",
    "print(normalize_data(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, weights ):\n",
    "    predictions = list()\n",
    "    for row in dataset:\n",
    "        values = 0.0\n",
    "        for i in range(len(weights)):\n",
    "            values += weights[i]*row[i];\n",
    "        predictions.append(values)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2000000000000002, 3.2, 4.0, 2.8000000000000003, 6.0]\n"
     ]
    }
   ],
   "source": [
    "def rmse(predictions, y):\n",
    "    error = 0.0\n",
    "    for i in range(len(y)):\n",
    "        error += (predictions[i] - y[i])**2\n",
    "    return error / len(y)\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "coef = [0.4, 0.8]\n",
    "print(predict(dataset,coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR_model(train_x, train_y, l_rate, n_epoch):\n",
    "    weights = init_weights(len(train_x))\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        predictions = predict(train_x, weights)\n",
    "        rms_error = rmse(predictions, train_y)\n",
    "        error = [0.0 for i in range(len(weights))]\n",
    "        for j in range(len(weights)):\n",
    "            error[j] = sum([(train_y[i] - predictions[i])* train[i][j] for i in range(len(train_x))])\n",
    "            weights[j] -= l_rate*error/len(train_x)\n",
    "        print('epoch no: ',epoch ,' weights: ', weights, 'error', rms_error)\n",
    "    return weights\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "file_name = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
